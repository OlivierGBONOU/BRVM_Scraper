# üèõÔ∏è BRVM Stock Data Scraper

[![Python](https://img.shields.io/badge/Python-3.7%2B-blue)](https://www.python.org/)
[![Selenium](https://img.shields.io/badge/Selenium-4.0%2B-green)](https://selenium.dev/)
[![License](https://img.shields.io/badge/License-MIT-yellow)](LICENSE)

Un outil complet de scraping des donn√©es boursi√®res de la BRVM (Bourse R√©gionale des Valeurs Mobili√®res) avec interface graphique moderne et fonctionnalit√©s avanc√©es.

## üìã Table des mati√®res

- [Aper√ßu](#-aper√ßu)
- [Fonctionnalit√©s](#-fonctionnalit√©s)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Configuration](#-configuration)
- [Structure du projet](#-structure-du-projet)
- [Donn√©es collect√©es](#-donn√©es-collect√©es)
- [D√©pannage](#-d√©pannage)
- [Contribution](#-contribution)
- [Licence](#-licence)

## üîç Aper√ßu

Ce projet permet d'extraire automatiquement les donn√©es historiques des actions cot√©es √† la BRVM via le site SikaFinance. Il propose deux modes d'utilisation :

- **Mode console** : Script Python autonome pour l'automatisation
- **Mode graphique** : Interface utilisateur intuitive avec Tkinter

### Capture d'√©cran

```
![alt text](image.png)
```

## ‚ú® Fonctionnalit√©s

### üéØ Scraping intelligent
- **Multi-actions** : Collecte automatique de toutes les actions disponibles
- **Gestion des p√©riodes** : Division intelligente des plages de dates
- **D√©tection des donn√©es** : √âvite les requ√™tes inutiles
- **Gestion d'erreurs** : R√©cup√©ration automatique et continuation

### üñ•Ô∏è Interface graphique
- **Configuration visuelle** : Param√©trage facile des dates et options
- **Suivi en temps r√©el** : Barre de progression et statuts d√©taill√©s
- **Journal int√©gr√©** : Logs complets avec horodatage
- **Export flexible** : Sauvegarde personnalis√©e des donn√©es

### üîß Options avanc√©es
- **Mode invisible** : Scraping en arri√®re-plan (headless)
- **Timeouts configurables** : Adaptation aux conditions r√©seau
- **Sauvegarde automatique** : Protection contre les pertes de donn√©es
- **Post-traitement** : Nettoyage et tri automatiques

## üöÄ Installation

### Pr√©requis
- Python 3.7 ou sup√©rieur
- Chrome ou Chromium install√©
- Connexion Internet stable

### Installation

```bash
# Cloner le repository
git clone https://github.com/OlivierGBONOU/Brvm-Scraper.git
cd Brvm-Scraper

# Lancer l'interface graphique
python brvm_gui.py
```

### D√©pendances

```txt
selenium
beautifulsoup4
pandas
webdriver-manager
```

## üìñ Utilisation

### Mode graphique (Recommand√©)

```bash
python brvm_gui.py
```

1. **Configurer les param√®tres** :
   - Date de d√©but et fin
   - Intervalle de scraping (jours)
   - Options avanc√©es

2. **Lancer le scraping** :
   - Cliquer sur "üöÄ D√©marrer le scraping"
   - Suivre la progression en temps r√©el

3. **Exporter les r√©sultats** :
   - Utiliser le bouton "üíæ Exporter CSV"
   - Choisir l'emplacement de sauvegarde

### Mode console

```python
from paste import BRVMScraper
from datetime import datetime

# Configuration
scraper = BRVMScraper({
    'headless': True,
    'timeout': 10
})

# Scraping
start_date = datetime(2023, 1, 1)
end_date = datetime.now()
result = scraper.scrape_data(start_date, end_date)

if result['success']:
    print(f"‚úÖ {result['records']} enregistrements collect√©s")
```

### Script automatis√©

```python
# Exemple de script pour automatisation
import schedule
import time
from datetime import datetime, timedelta

def scraping_quotidien():
    scraper = BRVMScraper({'headless': True})
    hier = datetime.now() - timedelta(days=1)
    aujourd_hui = datetime.now()
    
    result = scraper.scrape_data(hier, aujourd_hui)
    print(f"Scraping quotidien: {result}")

# Programmer le scraping quotidien √† 18h
schedule.every().day.at("18:00").do(scraping_quotidien)

while True:
    schedule.run_pending()
    time.sleep(3600)  # V√©rifier chaque heure
```

## ‚öôÔ∏è Configuration

### Param√®tres du scraper

```python
config = {
    'headless': True,          # Mode invisible
    'timeout': 10,             # Timeout en secondes
    'retry_attempts': 3,       # Tentatives de retry
    'interval_days': 30        # Intervalle par d√©faut
}
```

### Variables d'environnement

### Configuration avanc√©e

```python
# Options Chrome personnalis√©es
options = webdriver.ChromeOptions()
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--disable-gpu")
options.add_argument("--window-size=1920,1080")
```

## üìÅ Structure du projet

```
brvm-scraper/
‚îú‚îÄ‚îÄ BRVM_scraper.py       # Script de scraping principal
‚îú‚îÄ‚îÄ brvm_gui.py           # Interface graphique
‚îú‚îÄ‚îÄ README.md             # Documentation
‚îú‚îÄ‚îÄ data/                 # Dossier des donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ stock_data.csv    # Donn√©es finales
‚îÇ   ‚îî‚îÄ‚îÄ stock_data_temp.csv # Sauvegarde temporaire
‚îî‚îÄ‚îÄ logs/                 # Journaux d'ex√©cution
    ‚îî‚îÄ‚îÄ scraping.log
```

### Fichiers g√©n√©r√©s

- `stock_data.csv` : Donn√©es principales nettoy√©es
- `stock_data_temp.csv` : Sauvegarde pendant le scraping
- `stock_data_fallback.csv` : Sauvegarde d'urgence en cas d'erreur

## üìä Donn√©es collect√©es

### Colonnes disponibles

| Colonne | Description | Exemple |
|---------|-------------|---------|
| Date | Date de la s√©ance | 15/06/2024 |
| Ouverture | Prix d'ouverture | 1250.00 |
| Plus Haut | Prix maximum | 1280.00 |
| Plus Bas | Prix minimum | 1240.00 |
| Cl√¥ture | Prix de cl√¥ture | 1275.00 |
| Volume | Nombre de titres | 15420 |
| Capitalisation | Capitalisation boursi√®re | 2500000000 |
| ACTION | Code de l'action | BICC |

### Format des donn√©es

```csv
Date,Ouverture,Plus Haut,Plus Bas,Cl√¥ture,Volume,Capitalisation,ACTION
15/06/2024,1250.00,1280.00,1240.00,1275.00,15420,2500000000,BICC
14/06/2024,1245.00,1255.00,1235.00,1250.00,12580,2450000000,BICC
```

### Statistiques exemple

- **Actions disponibles** : ~40 titres
- **P√©riode couverte** : Depuis 2010
- **Fr√©quence** : Donn√©es quotidiennes
- **Volume typique** : 50 000 - 100 000 enregistrements/an

## üîß D√©pannage

### Probl√®mes courants

#### 1. ChromeDriver non trouv√©
```bash
# Solution : Installation automatique via webdriver-manager
# Incluse dans le script, aucune action requise
```

#### 2. Timeout de connexion
```python
# Augmenter le timeout dans la configuration
config = {'timeout': 30}  # Au lieu de 10
```

#### 3. Captcha ou blocage
```python
# Utiliser des d√©lais plus longs entre les requ√™tes
time.sleep(random.uniform(2, 5))
```

#### 4. Donn√©es manquantes
```python
# V√©rifier les logs pour identifier les p√©riodes probl√©matiques
# Le script reprend automatiquement les p√©riodes √©chou√©es
```

### Messages d'erreur

| Erreur | Cause | Solution |
|--------|-------|----------|
| `ElementNotFound` | Page non charg√©e | Augmenter timeout |
| `NoDataFound` | P√©riode sans donn√©es | Normal, continuer |
| `ConnectionError` | Probl√®me r√©seau | V√©rifier connexion |
| `ChromeDriverError` | Driver incompatible | Red√©marrer le script |

### Optimisation des performances

```python
# Configuration optimale pour gros volumes
config = {
    'headless': True,         # Plus rapide
    'timeout': 15,            # √âquilibre stabilit√©/vitesse
    'interval_days': 7,       # Requ√™tes plus petites
    'retry_attempts': 2       # Moins d'attente
}
```

## üìà Utilisation avanc√©e

### Analyse des donn√©es

```python
import pandas as pd
import matplotlib.pyplot as plt

# Charger les donn√©es
df = pd.read_csv('stock_data.csv')
df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')

# Analyse par action
for action in df['ACTION'].unique():
    action_data = df[df['ACTION'] == action]
    plt.figure(figsize=(12, 6))
    plt.plot(action_data['Date'], action_data['Cl√¥ture'])
    plt.title(f'√âvolution du cours - {action}')
    plt.show()
```

### Int√©gration dans un pipeline

```python
# Exemple d'int√©gration avec Apache Airflow
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

def daily_scraping():
    scraper = BRVMScraper({'headless': True})
    # Logique de scraping...

dag = DAG('brvm_daily_scraping', schedule_interval='@daily')
scraping_task = PythonOperator(
    task_id='scrape_brvm',
    python_callable=daily_scraping,
    dag=dag
)
```

### Export vers base de donn√©es

```python
import sqlite3

def save_to_database(csv_file):
    conn = sqlite3.connect('brvm_data.db')
    df = pd.read_csv(csv_file)
    df.to_sql('stock_prices', conn, if_exists='append', index=False)
    conn.close()
```

## ü§ù Contribution

### Guide de contribution

1. **Fork** le projet
2. **Cr√©er** une branche feature (`git checkout -b feature/AmazingFeature`)
3. **Commit** vos changements (`git commit -m 'Add AmazingFeature'`)
4. **Push** vers la branche (`git push origin feature/AmazingFeature`)
5. **Ouvrir** une Pull Request

### Standards de code

- **PEP 8** pour le style Python
- **Docstrings** pour toutes les fonctions
- **Tests unitaires** pour les nouvelles fonctionnalit√©s
- **Documentation** mise √† jour

### Signaler un bug

Utilisez les Issues GitHub avec :
- Description d√©taill√©e du probl√®me
- √âtapes pour reproduire
- Configuration syst√®me
- Logs d'erreur

## Version 1.0.0 (2024-06-16)